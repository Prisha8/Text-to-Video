{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:02.3603Z","iopub.execute_input":"2025-03-15T04:24:02.360534Z","iopub.status.idle":"2025-03-15T04:24:04.323071Z","shell.execute_reply.started":"2025-03-15T04:24:02.360513Z","shell.execute_reply":"2025-03-15T04:24:04.322381Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir groq\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:04.323771Z","iopub.execute_input":"2025-03-15T04:24:04.32409Z","iopub.status.idle":"2025-03-15T04:24:10.898777Z","shell.execute_reply.started":"2025-03-15T04:24:04.324072Z","shell.execute_reply":"2025-03-15T04:24:10.897698Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import groq\nimport requests \nimport json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:10.900009Z","iopub.execute_input":"2025-03-15T04:24:10.900359Z","iopub.status.idle":"2025-03-15T04:24:11.933986Z","shell.execute_reply.started":"2025-03-15T04:24:10.900326Z","shell.execute_reply":"2025-03-15T04:24:11.933276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"API_KEY = \"<YOUR_API_KEY>\"\n\nAPI_URL = \"<YOUR_API_KEY>  # Update to the correct Groq API endpoint\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:11.93475Z","iopub.execute_input":"2025-03-15T04:24:11.935001Z","iopub.status.idle":"2025-03-15T04:24:11.938383Z","shell.execute_reply.started":"2025-03-15T04:24:11.934981Z","shell.execute_reply":"2025-03-15T04:24:11.93758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nfrom groq import Groq\n\nclient = Groq(\n    api_key=\"<YOUR_API_KEY>\",\n)\n\nchat_completion = client.chat.completions.create(\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Explain the importance of fast language models\",\n        }\n    ],\n    model=\"<YOUR_API_KEY>\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:11.939348Z","iopub.execute_input":"2025-03-15T04:24:11.939626Z","iopub.status.idle":"2025-03-15T04:24:15.261611Z","shell.execute_reply.started":"2025-03-15T04:24:11.939599Z","shell.execute_reply":"2025-03-15T04:24:15.260507Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def groq_api_request(prompt, max_tokens=1500):\n    try:\n        response = client.chat.completions.create(\n            model=\"<YOUR_API_KEY>\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=max_tokens,\n        )\n        return response.choices[0].message.content  # Extracts the response text\n    except Exception as e:\n        print(f\" API Error: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:15.264549Z","iopub.execute_input":"2025-03-15T04:24:15.264784Z","iopub.status.idle":"2025-03-15T04:24:15.269079Z","shell.execute_reply.started":"2025-03-15T04:24:15.264762Z","shell.execute_reply":"2025-03-15T04:24:15.268229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"raw_text = \"\"\"\nMilitary Brilliance (1796-1797): Napoleon emerged as a strategic genius during his Italian Campaign, defeating Austrian forces and expanding French influence.\nCoup of 18 Brumaire (1799): He overthrew the Directory and established the Consulate, becoming First Consul of France.\nCoronation as Emperor (1804): He crowned himself Emperor, symbolizing his supreme authority over France.\nConquering Europe (1805-1812): Victories at Austerlitz, Jena, and Wagram cemented his dominance, reshaping the European political landscape.\nEconomic and Legal Reforms: The Napoleonic Code modernized laws, while infrastructure and education reforms strengthened France internally.\nFall:\n\nPeninsular War (1808-1814): Guerrilla warfare in Spain, backed by Britain, drained French resources and morale.\nRussian Campaign (1812): The disastrous invasion of Russia led to massive troop losses due to harsh winters and scorched-earth tactics.\nDefeat at Leipzig (1813): The Battle of Nations saw a coalition of European powers overpower Napoleon’s forces.\nFirst Exile to Elba (1814): He was forced to abdicate and exiled to Elba, but later escaped and briefly reclaimed power.\nWaterloo and Final Exile (1815): His defeat at Waterloo by the Duke of Wellington ended his rule, leading to his exile to Saint Helena, where he died in 1821.\n\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:15.270546Z","iopub.execute_input":"2025-03-15T04:24:15.270857Z","iopub.status.idle":"2025-03-15T04:24:15.291425Z","shell.execute_reply.started":"2025-03-15T04:24:15.270828Z","shell.execute_reply":"2025-03-15T04:24:15.290861Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ner_prompt = f\"\"\"\nYou are a professional NLP model. Extract structured entities from raw text using **Named Entity Recognition (NER)**.\nIdentify:\n\n- **Characters**: Names of people or fictional characters.\n- **Locations**: Places, cities, or fantasy settings.\n- **Objects**: Important items (e.g., \"ancient key\", \"magic book\").\n- **Events**: Key moments in the story.\n\n### Example Output:\n{{\n  \"characters\": [\"Alice\", \"The Shadow\"],\n  \"locations\": [\"Grand Library\"],\n  \"objects\": [\"Rusty Key\", \"Glowing Book\"],\n  \"events\": [\"Alice enters the library\", \"Alice finds the key\"]\n}}\n\n### Input:\n{raw_text}\n\nReturn the extracted **JSON output** in the above format.\n\"\"\"\n\n\nner_output = groq_api_request(ner_prompt)\nprint(\"Named Entity Recognition (NER) Output:\")\nprint(json.dumps(ner_output, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:15.292103Z","iopub.execute_input":"2025-03-15T04:24:15.292294Z","iopub.status.idle":"2025-03-15T04:24:16.2972Z","shell.execute_reply.started":"2025-03-15T04:24:15.292277Z","shell.execute_reply":"2025-03-15T04:24:16.296354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sentiment_prompt = f\"\"\"\nYou are an expert in **emotional tone analysis**.  \nAnalyze the input text and **detect sentiment shifts**.\n\n- **Identify tone changes** (e.g., hopeful → tense → fearful).  \n- **Classify emotions** (e.g., Joy, Fear, Suspense, Sadness).  \n- **Map emotions to specific parts** of the text.\n\n### Example Output:\n{{\n  \"sentiments\": [\n    {{\n      \"scene\": \"Alice Enters the Library\",\n      \"emotion\": \"Curiosity\",\n      \"text\": \"Alice cautiously steps inside, gripping an ancient key.\"\n    }},\n    {{\n      \"scene\": \"The Shadow Appears\",\n      \"emotion\": \"Fear\",\n      \"text\": \"A dark figure moves across the bookshelves, making Alice shiver.\"\n    }}\n  ]\n}}\n\n### Input:\n{raw_text}\n\nReturn the sentiment analysis **in JSON format**.\n\"\"\"\n\nsentiment_output = groq_api_request(sentiment_prompt)\nprint(\"Sentiment Analysis Output:\")\nprint(json.dumps(sentiment_output, indent=2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:16.298182Z","iopub.execute_input":"2025-03-15T04:24:16.298504Z","iopub.status.idle":"2025-03-15T04:24:19.03635Z","shell.execute_reply.started":"2025-03-15T04:24:16.298463Z","shell.execute_reply":"2025-03-15T04:24:19.035619Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dependency_prompt = f\"\"\"\nYou are an NLP expert specializing in **dependency parsing**.  \nYour goal is to extract structured dependency relationships from the input text.\n\n### Extraction Rules:\n- **Identify Subject-Verb-Object relationships** clearly.\n- **Detect cause-and-effect connections** in the text.\n- **Extract dependencies** influencing actions or decisions.\n- **Ensure a structured JSON output** with NO extra explanations.\n\n### Expected JSON Format:\n{{\n  \"dependencies\": [\n    {{\n      \"sentence\": \"...\",\n      \"subject\": \"...\",\n      \"verb\": \"...\",\n      \"object\": \"...\",\n      \"modifier\": \"...\",\n      \"cause\": \"...\",\n      \"effect\": \"...\"\n    }}\n  ]\n}}\n\n### Input:\n{raw_text}\n\n### Output:\n- Return a valid **JSON object ONLY**.  \n- No additional text, explanations, or formatting.\n\"\"\"\n\n\ndependency_output = groq_api_request(dependency_prompt)\nprint(\"\\n🟢 Dependency Parsing Output:\")\nprint(json.dumps(dependency_output, indent=2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:19.037064Z","iopub.execute_input":"2025-03-15T04:24:19.037265Z","iopub.status.idle":"2025-03-15T04:24:22.714292Z","shell.execute_reply.started":"2025-03-15T04:24:19.037247Z","shell.execute_reply":"2025-03-15T04:24:22.713492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nscript_prompt = scene_prompt = f\"\"\"\nYou are a professional scriptwriter. Generate a **multi-step screenplay** from the structured data below. Follow this process:\n\n1️⃣ **Set the Scene** → Describe the setting in cinematic detail.  \n2️⃣ **Introduce Characters** → Show their emotions and motivations.  \n3️⃣ **Develop Action** → Describe key events in a logical order.  \n4️⃣ **Ensure Coherence** → Maintain smooth transitions between scenes.  \n\n### Example Scene Breakdown:\n\n[SCENE 1]  \n**Location:** Grand Library  \n*A dimly lit hall with golden chandeliers. Alice steps inside, clutching an old key.*  \n\n**Alice** *(whispering)*  \n\"I hope this key still works...\"  \n\n*A shadow moves across the wall. Alice gasps, gripping the key tighter.*  \n\n---\n\n### Input Structured Data:\n{ner_output}\n\nReturn the **script in screenplay format**.\n\"\"\"\n\n\nscript_output = groq_api_request(script_prompt)\n\ntry:\n    \n    print(\" Script Generation Output:\")\n    print(script_output)\nexcept json.JSONDecodeError:\n    print(\" Error: Invalid JSON response from API\")\n    print(\"Raw API Output:\")\n    print(script_output)  # Print raw output for debugging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:22.715171Z","iopub.execute_input":"2025-03-15T04:24:22.715474Z","iopub.status.idle":"2025-03-15T04:24:27.910943Z","shell.execute_reply.started":"2025-03-15T04:24:22.715445Z","shell.execute_reply":"2025-03-15T04:24:27.910185Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# image_prompt = f\"\"\"\n# You are an AI visual artist. Generate **cinematic image descriptions** from this script.\n# - Maintain **consistent colors, lighting, and character design** across all scenes.  \n# - Describe **character expressions, posture, and mood** in each frame.  \n# - Ensure the **same visual style** throughout.  \n\n# ### Example:\n\n# **Image 1: Alice Enters the Library**  \n# \"A massive ancient library with golden chandeliers. Warm candlelight flickers across endless bookshelves. A young girl in a dark blue cloak, her brown hair tied back, holds a rusted key with glowing engravings. Her green eyes scan the room nervously.\"\n\n# ---\n\n# ### Input Script:\n# {script_output}\n\n# Return a **structured JSON list** of image prompts.\n# \"\"\"\n\n\n# image_prompt_output = groq_api_request(image_prompt)\n\n# try:\n    \n#     print(\" prompt  Generation Output:\")\n#     print(image_prompt_output)\n# except json.JSONDecodeError:\n#     print(\" Error: Invalid JSON response from API\")\n#     print(\"Raw API Output:\")\n#     print(script_output)  # Print raw output for debugging","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:27.911686Z","iopub.execute_input":"2025-03-15T04:24:27.911981Z","iopub.status.idle":"2025-03-15T04:24:27.916053Z","shell.execute_reply.started":"2025-03-15T04:24:27.911946Z","shell.execute_reply":"2025-03-15T04:24:27.915192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import json\n\n# def generate_scene_clusters(script_output):\n#     \"\"\"\n#     Segments a script into structured scene descriptions.\n\n#     Args:\n#         script_output (str): The raw script input.\n\n#     Returns:\n#         list: A list of scene clusters as JSON objects.\n#     \"\"\"\n#     if not script_output.strip():\n#         print(\"❌ Error: Script output is empty!\")\n#         return []\n\n#     # Scene segmentation LLM prompt\n#     segmentation_prompt = f\"\"\"\n#     You are a professional screenplay analyst. Your task is to break down the given script into **separate cinematic scenes**.\n    \n#     - Identify logical scene breaks based on **setting changes, major events, or shifts in time/mood**.\n#     - Ensure **each scene is a self-contained description** that can be used for visualization.\n#     - Keep descriptions **concise but vivid**, capturing the environment, characters, and action.\n#     - Return the results in a structured **JSON list**, where each scene has a unique number.\n\n#     ### Example Output:\n#     ```json\n#     [\n#         {{\"scene_number\": 1, \"scene_description\": \"A thunderstorm rages over an abandoned house. Lightning flashes, revealing a lone figure at the broken doorway.\"}},\n#         {{\"scene_number\": 2, \"scene_description\": \"A neon-lit alleyway in Tokyo. A detective in a trench coat exhales smoke, watching as a mysterious woman disappears into the shadows.\"}}\n#     ]\n#     ```\n\n#     ### Input Script:\n#     {script_output}\n\n#     Return only valid JSON.\n#     \"\"\"\n\n#     # Call the LLM API\n#     raw_response = groq_api_request(segmentation_prompt)\n\n#     try:\n        \n\n        \n\n#         return raw_response\n#         print(raw_response)\n\n#     except (json.JSONDecodeError, TypeError, ValueError) as e:\n#         print(f\"❌ Error: Invalid JSON response from LLM - {e}\")\n#         print(\"Raw API Output:\", raw_response)\n#         return []\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:27.917075Z","iopub.execute_input":"2025-03-15T04:24:27.917395Z","iopub.status.idle":"2025-03-15T04:24:27.943575Z","shell.execute_reply.started":"2025-03-15T04:24:27.917362Z","shell.execute_reply":"2025-03-15T04:24:27.942722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def generate_image_prompts(scene_clusters):\n#     \"\"\"\n#     Generates cinematic image descriptions from scene clusters.\n\n#     Args:\n#         scene_clusters (list): A list of segmented scene descriptions.\n\n#     Returns:\n#         list: A list of image descriptions (one per scene).\n#     \"\"\"\n#     image_prompts = []\n    \n#     for i, scene in enumerate(scene_clusters):\n#         image_prompt = f\"\"\"\n#         You are an AI visual artist. Generate a **cinematic image description** for the scene below.\n\n#         - Maintain **consistent colors, lighting, and character design** across all scenes.  \n#         - Describe **character expressions, posture, and mood** in each frame.  \n#         - Ensure the **same visual style** throughout.  \n\n#         ### Scene {i+1}:\n#         \"{scene}\"\n\n#         Return a **single JSON object** with the key `\"description\"`.\n#         \"\"\"\n\n#         raw_response = groq_api_request(image_prompt)\n\n#         # Validate the response\n#         try:\n#             script_output_json = json.loads(raw_response)\n#             description = script_output_json.get(\"description\", \"\")\n#             if description:\n#                 image_prompts.append(description)\n#         except (json.JSONDecodeError, TypeError) as e:\n#             print(f\"❌ Error: Invalid JSON response for scene {i+1} - {e}\")\n\n#     return image_prompts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:27.944568Z","iopub.execute_input":"2025-03-15T04:24:27.944893Z","iopub.status.idle":"2025-03-15T04:24:27.96452Z","shell.execute_reply.started":"2025-03-15T04:24:27.944865Z","shell.execute_reply":"2025-03-15T04:24:27.963898Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_scene_clusters(script_output):\n    \"\"\"\n    Segments a script into structured scene descriptions.\n\n    Args:\n        script_output (str): The raw script input.\n\n    Returns:\n        list: A list of scene clusters as JSON objects.\n    \"\"\"\n    segmentation_prompt = f\"\"\"\n    You are a professional screenplay analyst. Break the script into **scene clusters**.\n    - Identify logical scene breaks.\n    - Keep descriptions vivid but concise.\n    - Return a JSON list of scene clusters.\n\n    ### Input Script:\n    {script_output}\n\n    Return only valid JSON.\n    \"\"\"\n    \n    # Call your LLM API\n    raw_response = groq_api_request(segmentation_prompt)  \n\n    try:\n        print(raw_response)\n        print(3)\n        scene_clusters = json.loads(raw_response)\n        if not isinstance(scene_clusters, list):\n            raise ValueError(\"Invalid JSON format received from LLM.\")\n        return scene_clusters\n    except (json.JSONDecodeError, TypeError, ValueError) as e:\n        print(f\"❌ Error: Invalid JSON response - {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:27.965376Z","iopub.execute_input":"2025-03-15T04:24:27.965597Z","iopub.status.idle":"2025-03-15T04:24:27.988584Z","shell.execute_reply.started":"2025-03-15T04:24:27.965568Z","shell.execute_reply":"2025-03-15T04:24:27.987808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install diffusers transformers accelerate safetensors omegaconf\n!pip install controlnet_aux compel\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:27.98943Z","iopub.execute_input":"2025-03-15T04:24:27.98971Z","iopub.status.idle":"2025-03-15T04:24:36.705839Z","shell.execute_reply.started":"2025-03-15T04:24:27.989681Z","shell.execute_reply":"2025-03-15T04:24:36.70493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install torch torchvision torchaudio xformers\n!pip install opencv-python numpy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:36.706835Z","iopub.execute_input":"2025-03-15T04:24:36.707074Z","iopub.status.idle":"2025-03-15T04:24:44.882727Z","shell.execute_reply.started":"2025-03-15T04:24:36.707053Z","shell.execute_reply":"2025-03-15T04:24:44.881744Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableDiffusionPipeline\nimport torch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:24:44.883813Z","iopub.execute_input":"2025-03-15T04:24:44.884198Z","iopub.status.idle":"2025-03-15T04:25:22.183582Z","shell.execute_reply.started":"2025-03-15T04:24:44.88416Z","shell.execute_reply":"2025-03-15T04:25:22.182915Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"if torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:25:22.184325Z","iopub.execute_input":"2025-03-15T04:25:22.18479Z","iopub.status.idle":"2025-03-15T04:25:22.189544Z","shell.execute_reply.started":"2025-03-15T04:25:22.184768Z","shell.execute_reply":"2025-03-15T04:25:22.188609Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ.pop(\"HF_TOKEN\", None)  # Remove the existing token\n\nfrom huggingface_hub import login\nlogin(\"\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:25:22.193271Z","iopub.execute_input":"2025-03-15T04:25:22.193523Z","iopub.status.idle":"2025-03-15T04:25:22.340648Z","shell.execute_reply.started":"2025-03-15T04:25:22.193503Z","shell.execute_reply":"2025-03-15T04:25:22.340044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nos.environ[\"HF_TOKEN\"] = \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:25:22.341506Z","iopub.execute_input":"2025-03-15T04:25:22.341704Z","iopub.status.idle":"2025-03-15T04:25:22.345129Z","shell.execute_reply.started":"2025-03-15T04:25:22.341687Z","shell.execute_reply":"2025-03-15T04:25:22.344357Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport json\nimport matplotlib.pyplot as plt\nfrom diffusers import StableDiffusionPipeline\nfrom transformers import CLIPImageProcessor\n\n# Load Stable Diffusion Pipeline\npipeline = StableDiffusionPipeline.from_pretrained(\n    \"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16\n).to(\"cuda\")\n\ndef generate_scene_clusters(script_output):\n    \"\"\"\n    Segments a script into structured scene descriptions.\n\n    Args:\n        script_output (str): The raw script input.\n\n    Returns:\n        list: A list of scene clusters as JSON objects.\n    \"\"\"\n    segmentation_prompt = f\"\"\"\n    You are a professional screenplay analyst. Break the script into **scene clusters**.\n    - Identify logical scene breaks.\n    - Keep descriptions vivid but concise.\n    - Return a JSON list of scene clusters.\n\n    ### Input Script:\n    {script_output}\n\n    ### Output Format\n    cluster_id: <id>,\n    scene_id: <id>,\n    scene_description: <description>,\n    cluster_description: <description>\n\n    Return only valid JSON.\n    \"\"\"\n    \n    # Call your LLM API\n    raw_response = groq_api_request(segmentation_prompt)\n    try:\n        data = raw_response.replace(\"```json\", \"\").replace(\"```\",\"\")\n        scene_clusters = json.loads(data)\n        if not isinstance(scene_clusters, list):\n             raise ValueError(\"Invalid JSON format received from LLM.\")\n        return scene_clusters\n        #return raw_response\n    except (json.JSONDecodeError, TypeError, ValueError) as e:\n        print(f\"❌ Error: Invalid JSON response - {e}\")\n        return []\n\n\ndef generate_image_prompts(scene_clusters):\n    \"\"\"\n    Generates cinematic image descriptions from scene clusters.\n\n    Args:\n        scene_clusters (list): A list of segmented scene descriptions.\n\n    Returns:\n        list: A list of image descriptions (one per scene cluster).\n    \"\"\"\n    image_prompts = []\n    \n    for i, scene in enumerate(scene_clusters):\n        image_prompt = f\"\"\"\n        You are an AI visual artist. Generate a **cinematic image description** for the scene below.\n\n        - Maintain **consistent colors, lighting, and character design** across all scenes.\n        - Describe **character expressions, posture, and mood**.\n        - Ensure the **same visual style** throughout.\n\n        ### Scene {i+1}:\n        \"{scene['scene_description']}\"\n\n        Return a **single JSON object** with the key `\"description\"`.\n        \"\"\"\n\n        raw_response = groq_api_request(image_prompt)\n\n        try:\n            data = raw_response.replace(\"```json\", \"\").replace(\"```\",\"\")\n            response_json = json.loads(data)\n            description = response_json.get(\"description\", \"\")\n            if description:\n                image_prompts.append(description)\n            \n        except (json.JSONDecodeError, TypeError) as e:\n            print(f\"❌ Error: Invalid JSON response for scene {i+1} - {e}\")\n\n    return image_prompts\n\n\ndef generate_images_from_scenes(script_output):\n    \"\"\"\n    Processes a script, generates images for scene clusters, and plots them.\n\n    Args:\n        script_output (str): The raw script input.\n\n    Returns:\n        list: A list of generated images.\n    \"\"\"\n    \n    # Step 1: Get scene clusters\n    scene_clusters = generate_scene_clusters(script_output)\n\n    if not scene_clusters:\n        print(\"❌ No valid scene clusters generated.\")\n        return []\n\n    # Step 2: Generate image prompts\n    image_prompts = generate_image_prompts(scene_clusters)\n    print(image_prompts)\n    print(1)\n\n    if not image_prompts:\n        print(\"❌ No valid image prompts generated.\")\n        return []\n\n    # Step 3: Generate images\n    generated_images = []\n    print(len(generated_images))\n    previous_image = None  # For IP Adapter\n\n    fig, axs = plt.subplots(1, len(image_prompts), figsize=(15, 5))\n    print(image_prompts)\n\n    for i, prompt in enumerate(image_prompts):\n        print(f\"🎨 Generating Image {i+1}: {prompt}\")\n\n        generator = torch.Generator(device=\"cpu\").manual_seed(0)\n\n        # Generate first image normally\n        if previous_image is None:\n            image = pipeline(prompt=prompt, height=768, width=768, generator=generator).images[0]\n        else:\n            # Use previous image as context for the next one\n            pipeline.load_ip_adapter(\"h94/IP-Adapter\", subfolder=\"models\", weight_name=\"ip-adapter_sd15.bin\")\n            pipeline.set_ip_adapter_scale(0.6)\n\n            image = pipeline(prompt=prompt, ip_adapter_image=previous_image, num_inference_steps=100, generator=generator).images[0]\n\n        # Store image\n        generated_images.append(image)\n        previous_image = image  # Set for next iteration\n\n        # Plot image\n        axs[i].imshow(image)\n        axs[i].axis(\"off\")\n        axs[i].set_title(f\"Scene {i+1}\")\n\n    plt.show()\n    return generated_images\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:25:22.345777Z","iopub.execute_input":"2025-03-15T04:25:22.346Z","iopub.status.idle":"2025-03-15T04:25:45.684397Z","shell.execute_reply.started":"2025-03-15T04:25:22.345982Z","shell.execute_reply":"2025-03-15T04:25:45.683425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from diffusers import StableVideoDiffusionPipeline\nimport torch\nimport numpy as np\nimport cv2\nfrom IPython.display import display, Video\nfrom PIL import Image\n\n# Load model\nmodel_id = \"stabilityai/stable-video-diffusion-img2vid-xt\"\npipe = StableVideoDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, variant=\"fp16\").to(\"cuda\")\npipe.enable_model_cpu_offload()\ntorch.cuda.empty_cache()\n\ngenerator = torch.manual_seed(42)\nfinal_frames = []  # List to store final frames\n\n# Define the list of input images dynamically\ngenerated_images = generate_images_from_scenes(script_output)  # Replace with actual image variables\n\n# Process images dynamically\nframe_sequences = []  # Stores all frame sequences dynamically\n\nfor img in generated_images:\n    frames = pipe(\n        img,\n        decode_chunk_size=2,\n        generator=generator,\n        num_frames=len(generated_images)  # Dynamically setting number of frames\n    ).frames  # Use `.frames` instead of `.frames[0]` to get all frames\n\n    frame_sequences.append(frames)  # Store the sequence of frames\n\n# Transition frames count\nnum_transition_frames = 10\n\n# Generate transitions\nfor i in range(len(frame_sequences) - 1):  \n    frames_a = frame_sequences[i]  # Current scene frames\n    frames_b = frame_sequences[i + 1]  # Next scene frames\n\n    transition_frames = []\n\n    for alpha in np.linspace(0, 1, num_transition_frames):  \n        # Extract individual frames and ensure they are PIL images\n        img_a = frames_a[-1].convert(\"RGB\")  # Convert last frame of first sequence to RGB PIL Image\n        img_b = frames_b[0].convert(\"RGB\")   # Convert first frame of next sequence to RGB PIL Image\n\n        # Blend the images\n        blended_frame = Image.blend(img_a, img_b, alpha)\n        transition_frames.append(blended_frame)\n\n    # Add current scene frames and transition frames to final output\n    final_frames.extend(frames_a + transition_frames)\n\n# Append last scene frames\nfinal_frames.extend(frame_sequences[-1])\n\n# Save as video\n\n# Save as video\nvideo_path = \"\"\nfps = 7\n\n# Ensure correct shape\nheight, width = final_frames[0].size  # PIL size gives (width, height)\nfourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\nout = cv2.VideoWriter(video_path, fourcc, fps, (width, height))\n\nfor frame in final_frames:\n    frame = np.array(frame, dtype=np.uint8)  # Convert to uint8\n    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  # Convert for OpenCV\n    out.write(frame)\n\nout.release()\n\n# Display the video\ndisplay(Video(video_path, embed=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-15T04:25:45.685336Z","iopub.execute_input":"2025-03-15T04:25:45.685634Z","execution_failed":"2025-03-15T05:50:43.709Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}